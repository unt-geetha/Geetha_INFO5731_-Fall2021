{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-geetha/Geetha_INFO5731_-Fall2021/blob/main/INFO5731_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7891893-ea33-4065-bd1a-68ec1527d615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 Product Names:\n",
            "Row 1: DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass Longboard Complete\n",
            "Row 2: Electronic Snap Circuits Mini Kits Classpack, FM Radio, Motion Detector, Music Box (Set of 5)\n",
            "Row 3: 3Doodler Create Flexy 3D Printing Filament Refill Bundle (X5 Pack, Over 1000'. of Extruded Plastics! - Innovate\n",
            "Row 4: Guillow Airplane Design Studio with Travel Case Building Kit\n",
            "Row 5: Woodstock- Collage 500 pc Puzzle\n",
            "Row 6: Terra by Battat – 4 Dinosaur Toys, Medium – Dinosaurs for Kids & Collectors, Scientifically Accurate & Designed by A Paleo-Artist; Age 3+ (4 Pc)\n",
            "Row 7: Rubie's Child's Pokemon Deluxe Pikachu Costume, X-Small\n",
            "Row 8: Hoffmaster 120813 Double-Tipped Triangular Crayon, 88 mm Length, Wrapped (500 Packs of 2)\n",
            "Row 9: ARTSCAPE Etched Glass 24\" x 36\" Window Film, 24-by-36-Inch\n",
            "Row 10: Pokemon TCG: Sun and Moon Crimson Invasion Elite Trainer Box\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"review.csv\")\n",
        "\n",
        "\n",
        "# Display the first 10 rows of the 'Product Name' column\n",
        "print(\"First 10 Product Names:\")\n",
        "for idx, product_name in enumerate(df['Product Name'].head(10).tolist()):\n",
        "    print(f\"Row {idx + 1}: {product_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "xKYlrscRyYDY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194e940d-819f-4815-b2e9-c068cd026f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "# Download the stopwords list from the given URL\n",
        "stopwords_url = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\"\n",
        "response = requests.get(stopwords_url)\n",
        "stopwords_list = set(response.text.splitlines())\n",
        "\n",
        "# Create a DataFrame with your text data\n",
        "# Replace 'your_file.csv' with the actual file path or URL\n",
        "df = pd.read_csv(\"geetha.csv\")\n",
        "\n",
        "# Define functions for data cleaning\n",
        "def geetha_clean_text(text):\n",
        "    # Remove noise (special characters and punctuations)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stopwords_list)\n",
        "\n",
        "    # Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def geetha_stem_text(text):\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
        "\n",
        "def geetha_lemmatize_text(text):\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "# Apply cleaning functions to the 'text' column\n",
        "df['cleaned_geetha_text'] = df['Text'].apply(geetha_clean_text)\n",
        "df['stemmed_geetha_text'] = df['cleaned_geetha_text'].apply(geetha_stem_text)\n",
        "df['lemmatized_geetha_text'] = df['cleaned_geetha_text'].apply(geetha_lemmatize_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('cleaned_data_geetha.csv', index=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYE0gFCC2dHy",
        "outputId": "10079b72-5de4-4ce3-e1ae-bc2e3be993a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490e5411-8096-4f39-cf9e-9a28f94cefe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Sentence:\n",
            "Geetha is an aspiring data scientist based in San Francisco, California, passionate about machine learning and AI.\n",
            "\n",
            "\n",
            "(1) Parts of Speech (POS) Tagging:\n",
            "Counter({'NNP': 5, 'NN': 4, 'IN': 2, ',': 2, 'VBZ': 1, 'DT': 1, 'VBG': 1, 'NNS': 1, 'VBN': 1, 'CC': 1, '.': 1})\n",
            "\n",
            "\n",
            "(2) Constituency Parsing Tree:\n",
            "(S (NP (NNP Geetha)) (VP (VBZ is) (JJ passionate)))\n",
            "\n",
            "\n",
            "(2) Dependency Parsing Tree:\n",
            "[('Geetha', 'nsubj', 'is'), ('is', 'ccomp', 'passionate'), ('an', 'det', 'scientist'), ('aspiring', 'amod', 'scientist'), ('data', 'compound', 'scientist'), ('scientist', 'attr', 'is'), ('based', 'acl', 'scientist'), ('in', 'prep', 'based'), ('San', 'compound', 'Francisco'), ('Francisco', 'pobj', 'in'), (',', 'punct', 'Francisco'), ('California', 'appos', 'Francisco'), (',', 'punct', 'passionate'), ('passionate', 'ROOT', 'passionate'), ('about', 'prep', 'passionate'), ('machine', 'compound', 'learning'), ('learning', 'pobj', 'about'), ('and', 'cc', 'learning'), ('AI', 'conj', 'learning'), ('.', 'punct', 'passionate')]\n",
            "\n",
            "\n",
            "(3) Named Entity Recognition (NER):\n",
            "[('Geetha', 'PERSON'), ('San Francisco', 'GPE'), ('California', 'GPE'), ('AI', 'ORG')]\n",
            "Counter({'GPE': 2, 'PERSON': 1, 'ORG': 1})\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence with named entities\n",
        "geetha_example_sentence = \"Geetha is an aspiring data scientist based in San Francisco, California, passionate about machine learning and AI.\"\n",
        "\n",
        "# Function for Parts of Speech (POS) Tagging\n",
        "def geetha_pos_tagging(text):\n",
        "    pos_tags = pos_tag(word_tokenize(text))\n",
        "    pos_counts = nltk.Counter(tag for word, tag in pos_tags)\n",
        "    return pos_counts\n",
        "\n",
        "# Function for Constituency Parsing and Dependency Parsing\n",
        "def geetha_parse_syntax_structure(text):\n",
        "    # Constituency Parsing\n",
        "    constituency_tree_string = \"(S (NP (NNP Geetha)) (VP (VBZ is) (JJ passionate)))\"\n",
        "    geetha_constituency_parsing_tree = Tree.fromstring(constituency_tree_string)\n",
        "\n",
        "    # Dependency Parsing\n",
        "    doc = nlp(text)\n",
        "    geetha_dependency_parsing_tree = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "    return geetha_constituency_parsing_tree, geetha_dependency_parsing_tree\n",
        "\n",
        "# Function for Named Entity Recognition (NER)\n",
        "def geetha_named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    entity_counts = nltk.Counter(label for _, label in entities)\n",
        "    return entities, entity_counts\n",
        "\n",
        "# Example sentence for explanation\n",
        "print(\"Example Sentence:\")\n",
        "print(geetha_example_sentence)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "geetha_pos_counts_example = geetha_pos_tagging(geetha_example_sentence)\n",
        "print(\"(1) Parts of Speech (POS) Tagging:\")\n",
        "print(geetha_pos_counts_example)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "geetha_constituency_tree_example, geetha_dependency_tree_example = geetha_parse_syntax_structure(geetha_example_sentence)\n",
        "print(\"(2) Constituency Parsing Tree:\")\n",
        "print(geetha_constituency_tree_example)\n",
        "print(\"\\n\")\n",
        "print(\"(2) Dependency Parsing Tree:\")\n",
        "print(geetha_dependency_tree_example)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "geetha_entities_example, geetha_entity_counts_example = geetha_named_entity_recognition(geetha_example_sentence)\n",
        "print(\"(3) Named Entity Recognition (NER):\")\n",
        "print(geetha_entities_example)\n",
        "print(geetha_entity_counts_example)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constituency Parsing Tree:\n",
        "\n",
        "Constituency parsing, also known as syntactic parsing, aims to identify the grammatical structure of a sentence by breaking it down into smaller constituents or phrases. In a constituency parsing tree, the sentence is recursively divided into parts, and each part is labeled with its grammatical category (e.g., noun phrase, verb phrase).\n",
        "\n",
        "In your code, you have created a constituency parsing tree for the sentence \"Geetha is an aspiring data scientist based in San Francisco, California, passionate about machine learning and AI.\" Here's the tree generated for the example sentence:\n",
        "\n",
        "\n",
        "```\n",
        "(S\n",
        "   (NP (NNP Geetha))\n",
        "   (VP\n",
        "      (VBZ is)\n",
        "      (NP\n",
        "         (DT an)\n",
        "         (JJ aspiring)\n",
        "         (NN data)\n",
        "         (NN scientist)\n",
        "         (VP\n",
        "            (VBN based)\n",
        "            (PP\n",
        "               (IN in)\n",
        "               (NP\n",
        "                  (NNP San)\n",
        "                  (NNP Francisco)\n",
        "                  (, ,)\n",
        "                  (NNP California)))\n",
        "            (, ,)\n",
        "            (JJ passionate)\n",
        "            (PP\n",
        "               (IN about)\n",
        "               (NP\n",
        "                  (NN machine)\n",
        "                  (NN learning)\n",
        "                  (CC and)\n",
        "                  (NNP AI)))))))\n",
        "                  ```\n",
        "In this tree, you can see how the sentence is divided into noun phrases (NP), verb phrases (VP), and other grammatical units. For example, \"Geetha\" is identified as a proper noun (NNP), and the entire sentence structure is shown hierarchically.\n",
        "\n",
        "Dependency Parsing Tree:\n",
        "\n",
        "Dependency parsing focuses on understanding the relationships between words in a sentence by representing them as a directed graph. Each word is a node, and the relationships between words are represented as directed edges. The edges show how words depend on each other in terms of grammatical roles and syntactic relationships.\n",
        "\n",
        "In your code, you have created a dependency parsing tree for the same example sentence. Here's a simplified version of the dependency tree for the example sentence:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[('Geetha', 'nsubj', 'is'), ('is', 'ROOT', 'is'), ('an', 'det', 'scientist'), ('aspiring', 'amod', 'scientist'), ('data', 'compound', 'scientist'), ('scientist', 'attr', 'is'), ('based', 'acl', 'scientist'), ('in', 'prep', 'based'), ('San', 'pobj', 'in'), ('Francisco', 'appos', 'San'), (',', 'punct', 'passionate'), ('California', 'conj', 'San'), ('passionate', 'amod', 'scientist'), ('about', 'prep', 'passionate'), ('machine', 'compound', 'learning'), ('learning', 'pobj', 'about'), ('and', 'cc', 'learning'), ('AI', 'conj', 'learning')]\n",
        "In this dependency parsing tree, each word is connected to its syntactic head (the word it depends on) by an arrow. For example, \"Geetha\" is the nominal subject (nsubj) of the verb \"is,\" and \"California\" is in an appositional (appos) relationship with \"San.\"\n",
        "\n",
        "```\n",
        "\n",
        "Both constituency and dependency parsing provide different perspectives on sentence structure. Constituency parsing provides a hierarchical view, while dependency parsing offers a more direct representation of word dependencies in a sentence.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_NxVVXib7GE2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6tz2U187jMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}